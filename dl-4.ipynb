{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvRZRmMUijd/c+8IdeaVfP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize and Compare Activation Functions\n",
        "\n",
        "We'll train a **simple MLP** on **MNIST** using **Sigmoid**, **Tanh**, and **ReLU**.  \n",
        "We’ll compare:\n",
        "- Training speed (epochs to reach ~95% val accuracy)\n",
        "- Final accuracy\n",
        "- Gradient flow (via histogram of gradients)\n",
        "\n",
        "> **Minimal & Fast** – uses Keras, small network, early stopping."
      ],
      "metadata": {
        "id": "qmG56b0gTFKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "Shj1KFo8TIJ2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define MLP Model Factory"
      ],
      "metadata": {
        "id": "_7ZTALo4TJ-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp(activation):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation=activation, input_shape=(784,)),\n",
        "        layers.Dense(64, activation=activation),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "gMOtRQ9PTLQe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Models with Early Stopping\n",
        "We'll track:\n",
        "- `history` → loss/accuracy\n",
        "- `grad_norms` → average gradient norm per epoch (to detect vanishing/exploding)"
      ],
      "metadata": {
        "id": "nNufnH2dTMz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "histories = {}\n",
        "grad_norms = {}\n",
        "\n",
        "# Custom callback to record gradient norms\n",
        "class GradientLogger(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.norms = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Compute gradients using GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass on a batch (use first batch_size samples)\n",
        "            batch_x = self.model.input[0][:self.model.optimizer.batch_size or 32]\n",
        "            batch_y = self.model.input[1][:self.model.optimizer.batch_size or 32]\n",
        "            predictions = self.model(batch_x, training=True)\n",
        "            loss = self.model.compiled_loss(batch_y, predictions)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        grad_norms = [tf.norm(g).numpy() for g in grads if g is not None]\n",
        "        if grad_norms:\n",
        "            self.norms.append(np.mean(grad_norms))\n",
        "        else:\n",
        "            self.norms.append(0.0)\n",
        "\n",
        "# Re-define activations & histories (clear previous)\n",
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "histories = {}\n",
        "grad_norms = {}\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"\\nTraining with {act.upper()}...\")\n",
        "    model = create_mlp(act)\n",
        "\n",
        "    grad_logger = GradientLogger()\n",
        "\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=3, restore_best_weights=True, min_delta=0.001\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=128,\n",
        "        validation_split=0.2,\n",
        "        verbose=1,\n",
        "        callbacks=[early_stop, grad_logger]\n",
        "    )\n",
        "\n",
        "    histories[act] = history\n",
        "    grad_norms[act] = grad_logger.norms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "IcCcizGGTOCC",
        "outputId": "80c3e40b-0fc9-44e1-a362-b588b421c0bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with SIGMOID...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6339 - loss: 1.4447"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "The layer sequential has never been called and thus has no defined input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1772148978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1772148978.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Forward pass on a batch (use first batch_size samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: The layer sequential has never been called and thus has no defined input."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Speed & Accuracy"
      ],
      "metadata": {
        "id": "OCYYk9xwTO7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Accuracy over epochs\n",
        "plt.subplot(1, 3, 1)\n",
        "for act in activations:\n",
        "    val_acc = histories[act].history['val_accuracy']\n",
        "    plt.plot(val_acc, label=f'{act} (best: {max(val_acc):.3f})')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Training loss\n",
        "plt.subplot(1, 3, 2)\n",
        "for act in activations:\n",
        "    loss = histories[act].history['loss']\n",
        "    plt.plot(loss, label=act)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Gradient norm (log scale)\n",
        "plt.subplot(1, 3, 3)\n",
        "for act in activations:\n",
        "    norms = grad_norms[act]\n",
        "    plt.plot(norms, label=act)\n",
        "plt.yscale('log')\n",
        "plt.title('Avg Gradient Norm (log scale)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Gradient Norm')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L934i_36TQgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results Table"
      ],
      "metadata": {
        "id": "Te8K8dcRTSoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for act in activations:\n",
        "    h = histories[act]\n",
        "    best_val_acc = max(h.history['val_accuracy'])\n",
        "    epochs_trained = len(h.history['val_accuracy'])\n",
        "    final_grad_norm = grad_norms[act][-1] if grad_norms[act] else None\n",
        "    results.append({\n",
        "        'Activation': act,\n",
        "        'Best Val Acc': f'{best_val_acc:.4f}',\n",
        "        'Epochs': epochs_trained,\n",
        "        'Final Grad Norm': f'{final_grad_norm:.2e}' if final_grad_norm else 'N/A'\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ],
      "metadata": {
        "id": "wUMYAdEMTx8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}